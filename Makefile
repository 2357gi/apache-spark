WORKER=1
PYFILE=pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„

build_docker: Dockerfile
	@echo "\nğŸ“¦ build 2357gi/apache-spark"
	@docker build ./ -t 2357gi/apache-spark

build_compose:
	@echo "\nğŸ—  Pile up containers"
	@docker-compose up -d --scale worker=$(WORKER)

run: docker-compose.yml
	@echo "\nâœ¨ apache-spark cluster setup is start!"
	@make build_docker
	@make build_compose
	@echo "\nğŸ¾ set up is finished! \n you wanna access âš¡spaek shell, hit it! \n$$ make  s-shell"
	@echo "or you wanna start ğŸ“” pyspark notebook, hit it! \n$$ make notebook"
	@echo "... if u wanna open ğŸš shell? do this. \n$$ make shell"

s-shell:
	@echo "\nâš¡ start spark-shell...."
	@docker-compose exec master /spark/bin/spark-shell --master spark://localhost:7077

notebook:
	@echo "\nğŸ“” start pyspark notebook...."
	@docker-compose exec master /spark/bin/pyspark
	@docker-compose exec

shell:
	@echo "\nğŸš start master shell...."
	@docker-compose exec master /bin/bash


kick-py:
	@echo "\n ğŸ¯ lets kick spark with py-file!"
	@echo "\n target .py is ${PYFILE}"
	@docker-compose exec master /spark/bin/spark-submit --py-files ${PYFILE} ${PYFILE}

